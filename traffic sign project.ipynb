{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Here we are going to classify traffic sign images into 43 categories.\n        * HERE WE WILL USE CNN TO CLASSIFY THIS TRAFFIC SIGN IMAGES. ","execution_count":null},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.\nimport pickle\nimport matplotlib.pyplot as plt\nimport csv\nfrom math import sqrt, ceil\nfrom timeit import default_timer as timer\n\nfrom keras.utils.np_utils import to_categorical\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPool2D, AvgPool2D, BatchNormalization, Reshape\nfrom keras.preprocessing.image import ImageDataGenerator\nfrom keras.callbacks import LearningRateScheduler","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Understanding of Data\n    * Our data for training , validation and testing are in train.pickle , valid.pickle and test.pickle respectively hence we will use these three files to train our CNN model\n        1. train.pickle\n        2. valid.pickle\n        3. test.pickle\n* we will also use one more file in which our labels are stored.\n        4. label_names.csv\n # so lets start...\n  ## First we load our data and do preproccessing required.","execution_count":null},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"train_data = '/kaggle/input/traffic-signs-preprocessed/train.pickle'\nvalidate_data= '/kaggle/input/traffic-signs-preprocessed/valid.pickle'\ntest_data = '/kaggle/input/traffic-signs-preprocessed/test.pickle'\nwith open(train_data, mode='rb') as f:\n    train = pickle.load(f)\nwith open(test_data, mode='rb') as f:\n    test = pickle.load(f)\nwith open(validate_data, mode='rb') as f:\n    valid = pickle.load(f)\nX_train, y_train = train['features'], train['labels']\nX_valid, y_valid = valid['features'], valid['labels']\nX_test, y_test = test['features'], test['labels']\n# y_train = to_categorical(y_train, 43)\ny_valid = to_categorical(y_valid, 43)\n# print(y_train.shape)\nprint(X_train.shape,y_train.shape,X_valid.shape,y_valid.shape,X_test.shape,y_test.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"n_train = X_train.shape[0]\nn_test = X_test.shape[0]\nimage_shape = X_train.shape[1:]\n# n_classes = len(set(y_train))\n\nprint(\"training examples =\", n_train)\nprint(\"validation examples=\", X_valid.shape[0])\nprint(\"testing examples =\", n_test)\n# print(\"Number of output labels =\", n_classes)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Data visualizing\n* Here we plot our labels with there label name to understand data and further precess it. ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"figsize = plt.rcParams['figure.figsize']\nlabel_names={}\nwith open('/kaggle/input/traffic-signs-preprocessed/label_names.csv', 'r') as file:\n    temp = csv.reader(file)\n    for i in temp:\n        if i[0].isdigit():\n            label_names[int(i[0])] = i[1]\nn_classes = len(label_names)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def samples(features, labels):\n    hist =[0 for i in range(n_classes)]\n    sample = {}\n    for i, l in enumerate(labels):\n        hist[l] += 1\n        if l not in sample:\n            sample[l] = features[i]\n    return hist, sample","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def view(img, label=None):\n    if label:\n        print(balel_names[label])\n    if len(img.shape) == 3 and img.shape[2] == 3:\n        plt.imshow(img)\n    else:\n        plt.imshow(img.squeeze(), cmap='gray')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def data_viz(features, outputs):\n    plt.rcParams['figure.figsize'] = (20, 20)\n    hist1,sample = samples(features,outputs)\n    n_classes = len(set(outputs))\n    print('total features ',len(features),'total classes',n_classes)\n    fig,axs = plt.subplots(nrows=11, ncols=4)\n    i = 0\n    print('labels are as follows : ')\n    for raw in range(11):\n        for col in range(4):\n            ax = axs[raw][col]\n            ax.axis('off')\n            if i in sample:\n                ax.imshow(sample[i])\n            if i in label_names:\n                ax.set_title(\"No.{} {}(#{})\".format(i, label_names[i], hist1[i]), fontsize=12)\n            i += 1\n#     plt.rcParams['figure.figsize'] = fig\ndata_viz(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Make Model\n* Now we have been understand our data so lets make model for our data\n* Here we will use keras sequencial model for train our data","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"model = Sequential()\nmodel.add(Conv2D(filters=32, kernel_size=(5,5), activation='relu', input_shape=X_train.shape[1:]))\nmodel.add(Conv2D(filters=64, kernel_size=(5, 5), activation='relu'))\nmodel.add(MaxPool2D(pool_size=(2, 2)))\nmodel.add(Dropout(rate=0.25))\nmodel.add(Conv2D(filters=64, kernel_size=(5, 5), activation='relu'))\nmodel.add(MaxPool2D(pool_size=(2, 2)))\nmodel.add(Dropout(rate=0.25))\nmodel.add(Flatten())\nmodel.add(Dense(256, activation='relu'))\nmodel.add(Dropout(rate=0.5))\nmodel.add(Dense(43, activation='softmax'))\nmodel.compile(\n    loss='categorical_crossentropy', \n    optimizer='adam', \n    metrics=['accuracy']\n)\n#y_train=to_categorical(y_train)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# lets see how our model look like","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"model.summary()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Lets train our model over training dataset and validate it through validation dataset","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train, y_train = train['features'], train['labels']\nX_valid, y_valid = valid['features'], valid['labels']\nX_test, y_test = test['features'], test['labels']\ny_train = to_categorical(y_train, 43)\ny_test = to_categorical(y_test, 43)\ny_valid = to_categorical(y_valid, 43)\nprint(X_train.shape,y_train.shape,X_valid.shape,y_valid.shape,X_test.shape,y_test.shape)\n\n# print(X_train.shape)\n# print(y_train.shape)\nepochs = 15\nhistory = model.fit(X_train,y_train, batch_size=128, epochs=epochs,validation_data=(X_valid, y_valid))\n#history = model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Now we will plot our model on its Accuracy and Loss","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# print(history.history)\nplt.figure(0,figsize=(8,4))\nplt.plot(history.history['accuracy'], label='training accuracy')\nplt.plot(history.history['val_accuracy'], label='val accuracy')\nplt.title('Accuracy')\nplt.xlabel('epochs')\nplt.ylabel('accuracy')\nplt.legend()\nplt.figure(1,figsize=(8,4))\nplt.plot(history.history['loss'], label='training loss')\nplt.plot(history.history['val_loss'], label='val loss')\nplt.title('Loss')\nplt.xlabel('epochs')\nplt.ylabel('loss')\nplt.legend()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Save the Model","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"model.save('model.h5')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Now lets check for different size of filters\n## Here we will take filter sizes of:\n1. (3*3)\n2. (5*5)\n3. (7*7)\n4. (9*9)","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"filters = [3, 5, 7, 9]\nmodels = [0] * len(filters)\nfor i in range(len(models)):\n    models[i] = Sequential()\n    models[i].add(Conv2D(filters=32, kernel_size=filters[i], activation='relu', input_shape=X_train.shape[1:]))\n    models[i].add(Conv2D(filters=64, kernel_size=filters[i], activation='relu'))\n    models[i].add(MaxPool2D(pool_size=(2, 2)))\n    models[i].add(Dropout(rate=0.25))\n    models[i].add(Flatten())\n    models[i].add(Dense(256, activation='relu'))\n    models[i].add(Dropout(rate=0.5))\n    models[i].add(Dense(43, activation='softmax'))\n    models[i].compile(\n        loss='categorical_crossentropy', \n        optimizer='adam', \n        metrics=['accuracy']\n    )","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# lets train our model on different size of filters","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"hs = [0] * len(models)\nepochs=5\nfor i in range(len(hs)):\n    print('*******************   trained for ' + str (filters[i]) + ' * '+ str (filters[i]) + ' filter')\n    hs[i] = models[i].fit(X_train,y_train, batch_size=128, epochs=epochs,validation_data=(X_valid, y_valid))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Here we will plot how our model perform over different sizes of filters","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"fig=plt.figure(figsize=(8,6))\nplt.plot(hs[3].history['val_accuracy'], '-o',label=filters[3])\nplt.plot(hs[2].history['val_accuracy'], '-o',label=filters[2])\nplt.plot(hs[1].history['val_accuracy'], '-o',label=filters[1])\nplt.plot(hs[0].history['val_accuracy'], '-o',label=filters[0])\nplt.title('Accuracy for different size of filters ')\nplt.xlabel('epochs')\nplt.ylabel('Accuracy')\nplt.legend()\nfig=plt.figure(figsize=(8,6))\nplt.plot(hs[3].history['val_loss'], '-o',label=filters[3])\nplt.plot(hs[2].history['val_loss'], '-o',label=filters[2])\nplt.plot(hs[1].history['val_loss'], '-o',label=filters[1])\nplt.plot(hs[0].history['val_loss'], '-o',label=filters[0])\nplt.title('loss for different size of filters ')\nplt.xlabel('epochs')\nplt.ylabel('Loss')\nplt.legend()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Now we'll see our trained weights for different sizes of filters ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from math import sqrt,ceil\ndef convert_to_grid(x_input):\n    N, H, W, C = x_input.shape\n    grid_size = int(ceil(sqrt(N)))\n    grid_height = H * grid_size + 1 * (grid_size - 1)\n    grid_width = W * grid_size + 1 * (grid_size - 1)\n    grid = np.zeros((grid_height, grid_width, C)) + 255\n    next_idx = 0\n    y0, y1 = 0, H\n    for y in range(grid_size):\n        x0, x1 = 0, W\n        for x in range(grid_size):\n            if next_idx < N:\n                img = x_input[next_idx]\n                low, high = np.min(img), np.max(img)\n                grid[y0:y1, x0:x1] = 255.0 * (img - low) / (high - low)\n                next_idx += 1\n            x0 += W + 1\n            x1 += W + 1\n        y0 += H + 1\n        y1 += H + 1\n\n    return grid","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for i in range(len(models)):\n    w = models[i].get_weights()\n    print(w[0].shape)\n    temp = w[0].transpose(3, 0, 1, 2)\n    print(temp.shape) \n    fig = plt.figure()\n    grid = convert_to_grid(temp)\n    plt.imshow(grid.astype('uint8'), cmap='gray')\n    plt.axis('off')\n    plt.gcf().set_size_inches(10, 10)\n    name = 'Trained filters ' + str(filters[i]) + 'x' + str(filters[i])\n    plt.title(name, fontsize=18)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# finally we save our models","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"for i in range(len(models)):\n    t = 'model-' + str(filters[i]) + 'x' + str(filters[i]) + '.h5'\n    models[i].save(t)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}